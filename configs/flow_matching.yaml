data:
  dataset: "celeba"
  root: ./data/celeba-subset
  from_hub: false  # run "python download_dataset.py --output_dir YOUR_PATH" first if you want to use false here
  repo_name: "electronickale/cmu-10799-celeba64-subset"
  image_size: 64
  channels: 3
  num_workers: 8
  pin_memory: true
  augment: true

# Model architecture:
#   - set model.type: "dit" to train Flow Matching with Diffusion Transformer
#   - set model.type: "unet" to use the original U-Net
model:
  type: "dit"

  # DiT options:
  # Option A) preset variant from facebookresearch/DiT naming:
  dit_variant: null #"DiT-S/4"
  mlp_ratio: 4.0
  learn_sigma: false

  # Option B) custom DiT size (used if dit_variant is not set):
  patch_size: 2
  hidden_size: 384
  depth: 12
  num_heads: 6

  # U-Net options (used only when type == "unet"):
  base_channels: 128
  channel_mult: [1, 2, 2, 4]
  num_res_blocks: 2
  attention_resolutions: [16]
  num_heads: 4
  dropout: 0.1
  use_scale_shift_norm: true

# Training hyperparameters (same as DDPM for fair comparison)
training:
  batch_size: 32
  learning_rate: 1.0e-4
  warmup_steps: 5000
  peak_lr: 1.0e-4
  min_lr: 1.0e-6
  weight_decay: 0.01
  betas: [0.9, 0.999]
  ema_decay: 0.999
  ema_start: 1000
  gradient_clip_norm: 1.0
  num_iterations: 100000
  log_every: 1
  sample_every: 10000
  save_every: 10000
  num_samples: 64

# Flow matching specific parameters
flow_matching:
  num_inference_steps: 50  # Number of Euler integration steps during sampling
  sigma_min: 0.0  # Minimum noise level (0.0 for deterministic flow)

sampling:
  num_steps: 50  # Number of sampling steps (matches num_inference_steps)
  sampler: "flow_matching"

infrastructure:
  seed: 42
  device: "cuda"
  num_gpus: 1
  mixed_precision: true
  compile_model: false

checkpoint:
  dir: "./checkpoints"
  resume: null

logging:
  dir: "./logs"
  wandb:
    enabled: true
    project: "cmu-10799-diffusion"
    entity: null
